{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is [Reinforcement Learning](https://en.wikipedia.org/wiki/Reinforcement_learning) & Intro to Markov Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics Covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is RL\n",
    "* Markov Process\n",
    "* Markov Reward Process\n",
    "* Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is RL?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning is:      \n",
    "\n",
    "* A sub-field of machine learning that focuses on how a software agent takes actions in an environment to maximize a reward function\n",
    "* An approach that incorporates the time dimension of ML problems\n",
    "* A method that falls somewhere between supervised and unsupervised learning in terms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Markov Process](https://en.wikipedia.org/wiki/Markov_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a Markov Process we have a system that contains a set space $\\mathbf{S}$ that is finite, and a transition matrix $\\mathbf{T}$. In order for a system to be a Markov Process, it must statisfy the [Markov Property](https://en.wikipedia.org/wiki/Markov_property). The Markov Property states that the future system state depends only on the current state, and not any sequence of states. This type of system can be thought of as memoryless. I like to think of these as finite state machines (FSM) where each transition between two states has an associated probability of occuring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Transition Probability Matrix:    \n",
    " \n",
    "|   Current State    | sunny next | rainy next |\n",
    "| :---: | :---: | :---: |\n",
    "| sunny | 0.8   |  0.2  |\n",
    "| rainy | 0.1   | 0.9   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Markov Reward Process](https://en.wikipedia.org/wiki/Markov_reward_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov Reward Process builds off of a Markov Process by adding a reward $R$ to every state transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **reward** is just a scalar, that could be positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Reward Matrix:    \n",
    "\n",
    "|   Current State    | sunny next | rainy next |\n",
    "| :---: | :---: | :---: |\n",
    "| sunny | 0.5   |  -5  |\n",
    "| rainy | 5   | -2.2   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **return**, $G$, at time $t$ is the sum of subsequent rewards, where subsequent rewards are multiplied by a discount factor $\\gamma$ raised to the power of the index of the summation $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t + (k + 1)}$$ $$0 \\le \\gamma \\le 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition for gamma:   \n",
    "If gamma is 1, the return is the sum of all future rewards. This would correspond to perfect visibility of all future rewards.    \n",
    "if gamma is 0, the return is only the current reward. This would correspond to no insight into furture rewards.     \n",
    "Conventionally gamma is in between, like 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **value of state** is the mathematical expectation, or average of lots of possible chains. This provides a less volatile metric for reward at any given state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{V}(s) = \\mathbb{E}[ G | S_t = s ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Markov Decision Process](https://en.wikipedia.org/wiki/Markov_decision_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov Decision Process adds an `Action Dimension` to the transition matrix, which allows the agent to modify the probability of transition based on the action selected. The index of the `Action Dimension` is selected by the **policy**, which is the porbabilty of an action being selected given a curent state.A Markov Reward process can be thought of as a special case of and MDP, where the policy function is **fixed**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of an action occuring given a current state.          \n",
    "Policy Function:            \n",
    "      \n",
    "$$\\pi(a | s) = P[A_t = a | S_t = s] $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
