{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [OpenAI](http://www.openai.com) [Gym API](http://gym.openai.com/docs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topics Covered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Components of RL \n",
    "    * Agent\n",
    "    * Environment\n",
    "    * Actions\n",
    "    * Observations\n",
    "    * Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **agent** is a person or a thing that takes an active role. The agent is the implementor of the **policy** which decides what action to take at each time step. The agent decides what action to take based on the observation that it recieves from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a niave agent that makes a random choice regardless of observations\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "        current_obs = env.get_observation()\n",
    "        actions = env.get_actions()\n",
    "        reward = env.action(random.choice(actions))\n",
    "        self.total_reward += reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **environment** is a model of the world external to the **agent**. The environment is responsible for providing the agent with **observations** and **rewards**. The environment state will change depending on the agents actions.      \n",
    "OpenAI Environment class has 2 main attributes `action_space` and `observation_space` as well as two main methods `reset()` and `step()`. Each of the attributes represent their respective spaces. The reset method returns the environment to it's initial state. The step method is the central method of the Environment class and does the following things:               \n",
    "         \n",
    "* Takes an input that is the step to be taken and executes it\n",
    "* Gets new observations after this action\n",
    "* Gets the reward gained by this step\n",
    "* Provides and indication that the step is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.steps_left = 10\n",
    "\n",
    "    # observations will change based on agent behavior\n",
    "    # this informs the agents decisions\n",
    "    def get_observation(self):\n",
    "        return [0.0, 0.0, 0.0]\n",
    "    \n",
    "    # action set should likely change based on the agents actions\n",
    "    def get_actions(self):\n",
    "        return [0, 1]\n",
    "\n",
    "    # likely some 'win condition'\n",
    "    def is_done(self):\n",
    "        return self.steps_left == 0\n",
    "\n",
    "    def action(self, action):\n",
    "        if self.is_done():\n",
    "            raise Exception(\"Game is over\")\n",
    "        self.steps_left -= 1\n",
    "        return random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward: 3.4977\n"
     ]
    }
   ],
   "source": [
    "# object instantiation\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "# the NIAVE agent will make random choices for 10 steps\n",
    "while not env.is_done():\n",
    "    agent.step(env)\n",
    "\n",
    "print(\"Total reward: %.4f\" % agent.total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI ships with tons of pre-build environments to test on. A list can be found [here](https://gym.openai.com/envs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space can be either discrete or continous, or a combination of both. `Discrete Action Space` (pushing a button, moving in a grid) only one option is possible at time. `Continous Action Space` (run 9 degrees left, turn a nob 0-1). The environment could also have multiple actions that can be performed simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations are information that the `Environment` provides to the `Agent`. Observations can be as simple as a couple of numbers, or as complex as multiple videos or images.       \n",
    "OpenAI Observation types are as follows: Discrete, Box, Tuple. `Discrete` is a set of mutually exclusive possibilities. `Box` is an n-dimensional tensor. `Tuple` allows us to group together multiple space classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym Wrappers & Monitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just know that these exist and help extend OpenAI functionality in generic ways. [readthedocs.io](http://gym.openai.com/docs/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
